{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITeuXF1QrDpF",
        "outputId": "08d63d9c-55aa-455e-deea-73303c20ad03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Collecting easyocr\n",
            "  Downloading easyocr-1.7.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from easyocr) (1.13.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from easyocr) (0.24.0)\n",
            "Collecting python-bidi (from easyocr)\n",
            "  Downloading python_bidi-0.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from easyocr) (6.0.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.10/dist-packages (from easyocr) (2.0.6)\n",
            "Collecting pyclipper (from easyocr)\n",
            "  Downloading pyclipper-1.3.0.post6-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting ninja (from easyocr)\n",
            "  Downloading ninja-1.11.1.2-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2.36.1)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (2024.9.20)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->easyocr) (0.4)\n",
            "Downloading easyocr-1.7.2-py3-none-any.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.2-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.9/422.9 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyclipper-1.3.0.post6-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (912 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m912.2/912.2 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_bidi-0.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (286 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.8/286.8 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-bidi, pyclipper, ninja, easyocr\n",
            "Successfully installed easyocr-1.7.2 ninja-1.11.1.2 pyclipper-1.3.0.post6 python-bidi-0.6.3\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision easyocr opencv-python-headless"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TI8GveDUsKxa",
        "outputId": "37b8cd48-9061-437a-c878-76feb161380a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set paths for Drive\n",
        "DRIVE_BASE_PATH = '/content/drive/Shareddrives/avidea'\n",
        "TRAIN_DIR = os.path.join(DRIVE_BASE_PATH, 'Train')\n",
        "TEST_DIR = os.path.join(DRIVE_BASE_PATH, 'Test_data')\n",
        "OUTPUT_FILE = os.path.join(DRIVE_BASE_PATH, 'submission.csv')\n",
        "\n",
        "# Ensure directories exist\n",
        "os.makedirs(DRIVE_BASE_PATH, exist_ok=True)\n",
        "\n",
        "# Optional: Check GPU availability\n",
        "import torch\n",
        "print(\"GPU Available:\", torch.cuda.is_available())\n",
        "print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFf4Q__ZsrU8",
        "outputId": "ef90353c-7829-4e1f-e2a9-e943ff9f5d2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available: True\n",
            "GPU Name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ViT + data augmentation + OCR (Multi-Modal approch)"
      ],
      "metadata": {
        "id": "fWFA93WmQsyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import easyocr\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Configuration\n",
        "# TRAIN_DIR = '/kaggle/input/avidea-student-challenge/Train'\n",
        "# TEST_DIR = '/kaggle/input/avidea-student-challenge/Test'\n",
        "# OUTPUT_FILE = 'submission.csv'\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 1e-4\n",
        "EPOCHS = 10\n",
        "\n",
        "class LicensePlateEnhancer:\n",
        "    def __init__(self):\n",
        "        # Initialize EasyOCR reader for Arabic script\n",
        "        self.reader = easyocr.Reader(['ar'])\n",
        "\n",
        "    def preprocess_for_ocr(self, image):\n",
        "        \"\"\"\n",
        "        Preprocess image for better license plate detection\n",
        "        \"\"\"\n",
        "        # Convert to grayscale\n",
        "        gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "        # Apply adaptive thresholding\n",
        "        thresh = cv2.adaptiveThreshold(\n",
        "            gray, 255,\n",
        "            cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "            cv2.THRESH_BINARY, 11, 2\n",
        "        )\n",
        "\n",
        "        return thresh\n",
        "\n",
        "    def extract_license_plate(self, image):\n",
        "        \"\"\"\n",
        "        Attempt to extract and read license plate\n",
        "        \"\"\"\n",
        "        # Preprocess image\n",
        "        preprocessed = self.preprocess_for_ocr(image)\n",
        "\n",
        "        # Convert back to PIL Image for EasyOCR\n",
        "        ocr_image = Image.fromarray(preprocessed)\n",
        "\n",
        "        # Perform OCR\n",
        "        results = self.reader.readtext(np.array(ocr_image))\n",
        "\n",
        "        # Filter and process results\n",
        "        license_plates = [\n",
        "            result[1] for result in results\n",
        "            if len(result[1]) > 5  # Basic filtering for potential license plate text\n",
        "        ]\n",
        "\n",
        "        return license_plates[0] if license_plates else None\n",
        "\n",
        "# Data Transformations\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Additional data augmentation\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load Training Data\n",
        "train_data = torchvision.datasets.ImageFolder(TRAIN_DIR, transform=train_transforms)\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Load Test Data\n",
        "test_files = [os.path.join(TEST_DIR, f) for f in os.listdir(TEST_DIR) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "# Initialize License Plate Enhancer\n",
        "plate_recognizer = LicensePlateEnhancer()\n",
        "\n",
        "# Vision Transformer Model\n",
        "weights = ViT_B_16_Weights.DEFAULT\n",
        "model = vit_b_16(weights=weights)\n",
        "\n",
        "# Adapt the head for the number of classes\n",
        "num_classes = len(train_data.classes)\n",
        "in_features = model.heads.head.in_features\n",
        "model.heads.head = torch.nn.Linear(in_features, num_classes)\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "# Optimizer and Loss Function\n",
        "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = CrossEntropyLoss()\n",
        "\n",
        "# Training Loop\n",
        "def train_model():\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        accuracy = 100 * correct / total\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {running_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Prediction Function\n",
        "def predict_with_ocr():\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for file_path in tqdm(test_files, desc=\"Predicting\"):\n",
        "            # Open image\n",
        "            image = Image.open(file_path).convert(\"RGB\")\n",
        "\n",
        "            # Try to extract license plate first\n",
        "            license_plate = plate_recognizer.extract_license_plate(image)\n",
        "\n",
        "            if license_plate:\n",
        "                # If license plate found, search for matching class\n",
        "                matching_classes = [\n",
        "                    cls for cls in train_data.classes\n",
        "                    if license_plate in os.listdir(os.path.join(TRAIN_DIR, cls))\n",
        "                ]\n",
        "\n",
        "                if matching_classes:\n",
        "                    predicted_class = matching_classes[0]\n",
        "                else:\n",
        "                    # Fallback to Vision Transformer prediction\n",
        "                    tensor_image = test_transforms(image).unsqueeze(0).to(DEVICE)\n",
        "                    outputs = model(tensor_image)\n",
        "                    _, predicted_class_idx = torch.max(outputs, 1)\n",
        "                    predicted_class = train_data.classes[predicted_class_idx.item()]\n",
        "            else:\n",
        "                # No license plate found, use Vision Transformer\n",
        "                tensor_image = test_transforms(image).unsqueeze(0).to(DEVICE)\n",
        "                outputs = model(tensor_image)\n",
        "                _, predicted_class_idx = torch.max(outputs, 1)\n",
        "                predicted_class = train_data.classes[predicted_class_idx.item()]\n",
        "\n",
        "            predictions.append((os.path.basename(file_path), predicted_class))\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Main Execution\n",
        "def main():\n",
        "    # Train the model\n",
        "    train_model()\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = predict_with_ocr()\n",
        "\n",
        "    # Create submission file\n",
        "    submission = pd.DataFrame(predictions, columns=[\"Id\", \"class\"])\n",
        "    submission.to_csv(OUTPUT_FILE, index=False)\n",
        "\n",
        "    print(f\"Submission saved to {OUTPUT_FILE}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1I5JQ-DsKt8",
        "outputId": "73eae596-b280-4c6b-b398-bc42892ebd47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Downloading detection model, please wait. This may take several minutes depending upon your network connection.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:easyocr.easyocr:Downloading recognition model, please wait. This may take several minutes depending upon your network connection.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: |██████████████████████████████████████████████████| 100.0% Complete"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n",
            "100%|██████████| 330M/330M [00:03<00:00, 112MB/s]\n",
            "Epoch 1/10: 100%|██████████| 53/53 [09:20<00:00, 10.58s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 301.3429, Accuracy: 1.25%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|██████████| 53/53 [01:40<00:00,  1.89s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10, Loss: 240.8746, Accuracy: 20.13%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|██████████| 53/53 [01:38<00:00,  1.85s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10, Loss: 146.5671, Accuracy: 71.26%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|██████████| 53/53 [01:42<00:00,  1.93s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10, Loss: 76.8532, Accuracy: 94.30%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 53/53 [01:39<00:00,  1.88s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10, Loss: 33.9659, Accuracy: 99.05%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|██████████| 53/53 [01:41<00:00,  1.91s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10, Loss: 14.3978, Accuracy: 99.76%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 53/53 [01:39<00:00,  1.88s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10, Loss: 6.8420, Accuracy: 99.88%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 53/53 [01:40<00:00,  1.89s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10, Loss: 4.0881, Accuracy: 99.88%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 53/53 [01:39<00:00,  1.87s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10, Loss: 2.9812, Accuracy: 99.88%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 53/53 [01:40<00:00,  1.89s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Loss: 2.3642, Accuracy: 99.88%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting: 100%|██████████| 511/511 [05:48<00:00,  1.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission saved to /content/drive/Shareddrives/avidea/submission.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ResNet50 + OCR + Data augmentation"
      ],
      "metadata": {
        "id": "cJ7bEOsiRoNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install albumentations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zil_ZZU5fNsg",
        "outputId": "b427e6ff-d7cf-4ef0-c87d-dcc75747ca6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (1.4.20)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (1.13.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from albumentations) (2.10.3)\n",
            "Requirement already satisfied: albucore==0.0.19 in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.0.19)\n",
            "Requirement already satisfied: eval-type-backport in /usr/local/lib/python3.10/dist-packages (from albumentations) (0.2.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.10/dist-packages (from albumentations) (4.10.0.84)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.10/dist-packages (from albucore==0.0.19->albumentations) (3.11.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.7.0->albumentations) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models\n",
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import easyocr\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "SEED = 42\n",
        "DRIVE_BASE_PATH = '/content/drive/Shareddrives/avidea'\n",
        "TRAIN_DIR = os.path.join(DRIVE_BASE_PATH, 'Train')\n",
        "TEST_DIR = os.path.join(DRIVE_BASE_PATH, 'Test_data')\n",
        "OUTPUT_FILE = os.path.join(DRIVE_BASE_PATH, 'submission4.csv')\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 1e-4\n",
        "EPOCHS = 10\n",
        "\n",
        "class LicensePlateEnhancer:\n",
        "    def __init__(self):\n",
        "        # Initialize EasyOCR reader for Arabic script\n",
        "        self.reader = easyocr.Reader(['ar'])\n",
        "\n",
        "    def preprocess_for_ocr(self, image):\n",
        "        \"\"\"\n",
        "        Preprocess image for better license plate detection\n",
        "        \"\"\"\n",
        "        # Convert to grayscale\n",
        "        gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "        # Apply adaptive thresholding\n",
        "        thresh = cv2.adaptiveThreshold(\n",
        "            gray, 255,\n",
        "            cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "            cv2.THRESH_BINARY, 11, 2\n",
        "        )\n",
        "\n",
        "        return thresh\n",
        "\n",
        "    def extract_license_plate(self, image):\n",
        "        \"\"\"\n",
        "        Attempt to extract and read license plate\n",
        "        \"\"\"\n",
        "        # Preprocess image\n",
        "        preprocessed = self.preprocess_for_ocr(image)\n",
        "\n",
        "        # Convert back to PIL Image for EasyOCR\n",
        "        ocr_image = Image.fromarray(preprocessed)\n",
        "\n",
        "        # Perform OCR\n",
        "        results = self.reader.readtext(np.array(ocr_image))\n",
        "\n",
        "        # Filter and process results\n",
        "        license_plates = [\n",
        "            result[1] for result in results\n",
        "            if len(result[1]) > 5  # Basic filtering for potential license plate text\n",
        "        ]\n",
        "\n",
        "        return license_plates[0] if license_plates else None\n",
        "\n",
        "# Data Transformations\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # Additional data augmentation\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load Training Data\n",
        "train_data = torchvision.datasets.ImageFolder(TRAIN_DIR, transform=train_transforms)\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Load Test Data\n",
        "test_files = [os.path.join(TEST_DIR, f) for f in os.listdir(TEST_DIR) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "\n",
        "# Initialize License Plate Enhancer\n",
        "plate_recognizer = LicensePlateEnhancer()\n",
        "\n",
        "# ResNet-50 Model\n",
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "# Adapt the head for the number of classes\n",
        "num_classes = len(train_data.classes)\n",
        "in_features = model.fc.in_features\n",
        "model.fc = torch.nn.Linear(in_features, num_classes)\n",
        "\n",
        "# Move model to device\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "# Optimizer and Loss Function\n",
        "optimizer = Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "criterion = CrossEntropyLoss()\n",
        "\n",
        "# Training Loop\n",
        "def train_model():\n",
        "    for epoch in range(EPOCHS):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
        "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        accuracy = 100 * correct / total\n",
        "        print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {running_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "# Prediction Function\n",
        "def predict_with_ocr():\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for file_path in tqdm(test_files, desc=\"Predicting\"):\n",
        "            # Open image\n",
        "            image = Image.open(file_path).convert(\"RGB\")\n",
        "\n",
        "            # Try to extract license plate first\n",
        "            license_plate = plate_recognizer.extract_license_plate(image)\n",
        "\n",
        "            if license_plate:\n",
        "                # If license plate found, search for matching class\n",
        "                matching_classes = [\n",
        "                    cls for cls in train_data.classes\n",
        "                    if license_plate in os.listdir(os.path.join(TRAIN_DIR, cls))\n",
        "                ]\n",
        "\n",
        "                if matching_classes:\n",
        "                    predicted_class = matching_classes[0]\n",
        "                else:\n",
        "                    # Fallback to ResNet-50 prediction\n",
        "                    tensor_image = test_transforms(image).unsqueeze(0).to(DEVICE)\n",
        "                    outputs = model(tensor_image)\n",
        "                    _, predicted_class_idx = torch.max(outputs, 1)\n",
        "                    predicted_class = train_data.classes[predicted_class_idx.item()]\n",
        "            else:\n",
        "                # No license plate found, use ResNet-50\n",
        "                tensor_image = test_transforms(image).unsqueeze(0).to(DEVICE)\n",
        "                outputs = model(tensor_image)\n",
        "                _, predicted_class_idx = torch.max(outputs, 1)\n",
        "                predicted_class = train_data.classes[predicted_class_idx.item()]\n",
        "\n",
        "            predictions.append((os.path.basename(file_path), predicted_class))\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Main Execution\n",
        "def main():\n",
        "    # Train the model\n",
        "    train_model()\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = predict_with_ocr()\n",
        "\n",
        "    # Create submission file\n",
        "    submission = pd.DataFrame(predictions, columns=[\"Id\", \"class\"])\n",
        "    submission.to_csv(OUTPUT_FILE, index=False)\n",
        "\n",
        "    print(f\"Submission saved to {OUTPUT_FILE}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKOLPJBLPfan",
        "outputId": "d6afee35-d85d-40f9-e9b0-83b3820b74b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 150MB/s]\n",
            "Epoch 1/10: 100%|██████████| 53/53 [00:59<00:00,  1.13s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 300.3916, Accuracy: 3.09%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|██████████| 53/53 [00:57<00:00,  1.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10, Loss: 243.8261, Accuracy: 17.04%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|██████████| 53/53 [00:57<00:00,  1.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10, Loss: 191.5872, Accuracy: 41.45%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|██████████| 53/53 [00:57<00:00,  1.08s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10, Loss: 143.0513, Accuracy: 61.34%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 53/53 [00:56<00:00,  1.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10, Loss: 100.0921, Accuracy: 79.28%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|██████████| 53/53 [00:56<00:00,  1.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10, Loss: 66.8191, Accuracy: 89.96%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 53/53 [00:56<00:00,  1.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10, Loss: 42.3251, Accuracy: 96.02%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 53/53 [00:56<00:00,  1.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10, Loss: 25.2716, Accuracy: 98.75%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 53/53 [00:57<00:00,  1.09s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10, Loss: 15.4374, Accuracy: 99.52%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 53/53 [00:56<00:00,  1.07s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Loss: 9.8594, Accuracy: 99.70%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Predicting: 100%|██████████| 511/511 [03:27<00:00,  2.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission saved to /content/drive/Shareddrives/avidea/submission4.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}